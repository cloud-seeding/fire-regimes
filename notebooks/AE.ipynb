{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import decomposition\n",
    "from sklearn.manifold import TSNE\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_areas = pd.read_csv('~/Desktop/columbia/capstone/fire-regimes/data/profiles-areas.csv')\n",
    "X = profiles_areas.drop(columns=['_uid_','initialdat','finaldate'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(X)\n",
    "\n",
    "X_scaled = X[~(np.abs(scaled_data) > 10).any(axis=1)]\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An AutoEncoder Class with dynamic latent dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        #Encoder\n",
    "        encoder_layers = []\n",
    "        input_size = 174\n",
    "        for layer_size in layers:\n",
    "            encoder_layers.append(torch.nn.Linear(input_size, layer_size))\n",
    "            encoder_layers.append(torch.nn.ReLU())\n",
    "            input_size = layer_size\n",
    "        self.encoder = torch.nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        #Decoder\n",
    "        decoder_layers = []\n",
    "        reversed_layers = layers[::-1]\n",
    "        for layer_size in reversed_layers[1:]:\n",
    "            decoder_layers.append(torch.nn.Linear(input_size, layer_size))\n",
    "            decoder_layers.append(torch.nn.ReLU())\n",
    "            input_size = layer_size\n",
    "        decoder_layers.append(torch.nn.Linear(input_size, 174))\n",
    "        self.decoder = torch.nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_loader,n_epochs,optimizer,loss_function):\n",
    "\n",
    "  losses = []\n",
    "\n",
    "  for _ in range(n_epochs):\n",
    "    for profile in train_loader:\n",
    "        \n",
    "      # Output of Autoencoder\n",
    "      reconstructed = model(profile)\n",
    "        \n",
    "      # Calculating the loss function\n",
    "      loss = loss_function(reconstructed, profile)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "        \n",
    "      losses.append(loss.item())\n",
    "  \n",
    "  return (model,losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_average(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "def plot_loss(losses,window_size):\n",
    "    loss_floats = [loss for loss in losses]\n",
    "    smoothed_losses = rolling_average(loss_floats, window_size)\n",
    "    plt.plot(smoothed_losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.from_numpy(X_scaled).float()\n",
    "train_data = train_data[:, :-1]\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent Dim: 4, Avg Validation Loss: 6.233581316412536e-07\n",
      "Latent Dim: 6, Avg Validation Loss: 5.260538010285384e-07\n",
      "Latent Dim: 8, Avg Validation Loss: 4.7482631847733987e-07\n",
      "Latent Dim: 12, Avg Validation Loss: 6.272702753668034e-07\n",
      "Latent Dim: 16, Avg Validation Loss: 6.781426023215171e-07\n",
      "Latent Dim: 24, Avg Validation Loss: 5.517599806729095e-07\n",
      "Latent Dim: 32, Avg Validation Loss: 4.95390734594458e-07\n",
      "Latent Dim: 48, Avg Validation Loss: 5.867710998081921e-07\n"
     ]
    }
   ],
   "source": [
    "latent_dims = [4,6,8,12,16,24,32,48]\n",
    "results = []\n",
    "\n",
    "for i,latent_dim in enumerate(latent_dims):\n",
    "    fold_losses = []\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(train_data):\n",
    "        # Split data\n",
    "        train_subset = torch.utils.data.Subset(train_data, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(train_data, val_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "        # Initialize model, optimizer, and loss function for this fold\n",
    "        model = AutoEncoder(latent_dim=latent_dim)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-8)\n",
    "        loss_function = torch.nn.MSELoss()\n",
    "\n",
    "        # Train model and get average training loss for the fold\n",
    "        avg_train_loss = train_model(model, train_loader=train_loader, n_epochs=2, optimizer=optimizer, loss_function=loss_function)\n",
    "\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for profile in val_loader:\n",
    "                reconstructed = model(profile)\n",
    "                val_loss = loss_function(reconstructed, profile)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        fold_losses.append(avg_val_loss)\n",
    "\n",
    "    # Record average validation loss for this latent dimension\n",
    "    results.append(np.mean(fold_losses))\n",
    "    print(f\"Latent Dim: {latent_dim}, Avg Validation Loss: {results[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(layers=[128,64,16,6])\n",
    " \n",
    "# Validation using MSE Loss function\n",
    "loss_function = torch.nn.MSELoss()\n",
    " \n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr = 0.01,\n",
    "                             weight_decay = 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,losses = train_model(model,n_epochs=2,train_loader=train_loader,loss_function=loss_function,optimizer=optimizer)\n",
    "plot_loss(losses=losses,window_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../assets/AE-6d.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model(train_data).detach().numpy()\n",
    "train = train_data.numpy()\n",
    "l = model.encoder(train_data).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(pd.DataFrame(l))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
